from __gin__ import dynamic_registration
import tasks
import __main__ as train_script
import seqio
from t5.data import mixtures
from t5x import models
from t5x import partitioning
from t5x import utils

# import modified implementations
include "t5x/examples/decoder_only/models/large.gin"
include "t5x/configs/runs/pretrain.gin"

MIXTURE_OR_TASK_NAME = "bt5"
TASK_FEATURE_LENGTHS = {"targets": 1024}
TRAIN_STEPS = 100000
BATCH_SIZE = 128 # no of sequences
USE_CACHED_TASKS = False

DROPOUT_RATE = 0.0

seqio.SentencePieceVocabulary.sentencepiece_model_file = "gs://bt5-small-bucket/new_vocabs/50k/new_vocab.model"

train_script.train:
  random_seed = 42
  train_eval_dataset_cfg = None
  infer_eval_dataset_cfg = None

utils.SaveCheckpointConfig:
  period = 50000  # checkpoint frequency

utils.RestoreCheckpointConfig:
  path = []  # initialize from scratch

# Follow the PaLM learning schedule
utils.create_learning_rate_scheduler:
  factors = 'constant * rsqrt_decay'
  base_learning_rate = 1.0
  warmup_steps = 10000